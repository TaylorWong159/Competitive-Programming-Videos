\documentclass{article}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{array}
\usepackage{verbatim}

\geometry{
total={160mm,257mm},
left=20mm,
top=15mm,
bottom=20mm
}

\begin{document}
\begin{center}
    {\huge Range Query}

    Author: Taylor Wong
\end{center}

In this video, we're going to explore a family of problems known as range queries problems along with the data structures used to solve them efficiently. Including, Prefix Arrays, Square Root Decomposition, Fenwick Trees, Sparse Tables, and Segment Trees. Along the way, we'll be building the intuition behind the ideas that make these data structures work.

Say we have an array of $n$ integers $A$ and we're tasked with determining the sum of the integers in $A$ within a range of indices $[l, r]$. One way to do this is to simply iterate through the array from index $l$ to $r$ and accumulate the sum. In the worst case, this approach takes $O(n)$ time when $l = 0$ and $r = n - 1$. Since we need to read all $n$ elements, this is optimal and has minimal overhead. However, what if we modify the problem slightly? Suppose we have to answer multiple range sum queries on the same array $A$. Say we have $q$ queries, each asking for the sum of elements between indices $l_i$ and $r_i$. If we use the naive approach for each query, the total time complexity would be $O(qn)$ in the worst case, which gets much worse as the number of queries $q$ increases. It's also important to note that $q$ can be as large as $O(n^2)$ since there are $O(n^2)$ possible ranges in an array of size $n$, so $q$ will dominate $n$ but we will still express our final time complexity in terms of both $n$ and $q$ since not all problems will allow $q$ to get that large. Additionally, since any approach must answer all $q$ queries, instead of measuring the overall time complexity, we will measure the average time complexity per query. In this case, the average time complexity per query using the naive approach is $O(n)$.

One way we often optimize these kinds of problems is by using precomputation. For example, we can create an $n \times n$ table where the entry at row $i$ and column $j$ stores the sum of elements from index $i$ to index $j$ in the array $A$. Now, if we fill the table out naively we would still take $O(n^3)$ time, since there are $O(n^2)$ entries in the table and each entry takes $O(n)$ time to compute. However, we can fill out the table more cleverly. Notice that the sum from index $i$ to index $j$ can be computed as the sum from index $i$ to index $j-1$ plus the element at index $j$. This means we can extend a row of the table in $O(1)$ time per entry. Thus, we can fill out the entire table in $O(n^2)$ time. Once the table is filled out, we can answer each range sum query in $O(1)$ time by simply looking up the corresponding entry in the table. While this is a massive improvement over the naive $O(n)$ approach, this method uses $O(n^2)$ space to store the table, which can be make this a non-starter for even $n=50000$.

To improve further we can use a similar property as we did when filling out the table. Much like how the sum from index $i$ to index $j$ can be computed from the sum from index $i$ to index $j-1$ and adding the element at index $j$, we can also compute it from the sum from index $i-1$ to index $j$ and subtracting the element at index $i-1$. Now this might seem worse since it requires computing a larger range sum but if we generalize further, we can see that the sum from $i$ to $j$ can be computed by taking the sum from $i-k$ to $j$ and subtracting the sum from $i-k$ to $i-1$ for any $k <= i$. Particularly if we let $k = i$, we see that the sum from index $i$ to index $j$ can be computed as the sum from index $0$ to index $j$ minus the sum from index $0$ to index $i-1$. Now since both of these start from the same index $0$ we can use the same precomputational approach as before but we only need one row of the table (namely the first one). This gives us an array which we call a prefix array $P$ where the entry at index $i$ stores the sum of elements from index $0$ to index $i$ in the array $A$. We can fill out this prefix sum array in $O(n)$ time just as before. Once we have the prefix array, we can answer each range sum query in $O(1)$ time by computing the difference between entries $j$ and $i - 1$ in the prefix sum array. Of course there is an edge case where $i=0$. However, we can just return entry $j$ since the $j^{th}$ entry of the prefix array by definition stores the sum from $0$ to $j$. Thus, our queries can again be answered in $O(1)$ time but now we're only using $O(n)$ space for the prefix sum array, making it much more space-efficient than the previous method.

Now prefix arrays are actually optimal for this problem in terms of both time and space complexity, since we have to answer all queries and we need $O(n)$ space to store the array itself.


Let's look at an example. Suppose we have the array $A = [3, 2, -1, 6, 5]$ and $q = 3$ queries:
\begin{enumerate}
    \item $[1, 3]$
    \item $[0, 4]$
    \item $[2, 2]$
\end{enumerate}
First, we compute the prefix sum array $P$:
\begin{itemize}
    \item $P[0] = A[0] = 3$
    \item $P[1] = P[0] + A[1] = 3 + 2 = 5$
    \item $P[2] = P[1] + A[2] = 5 + (-1) = 4$
    \item $P[3] = P[2] + A[3] = 4 + 6 = 10$
    \item $P[4] = P[3] + A[4] = 10 + 5 = 15$
\end{itemize}
Now, we can answer each query using the prefix sum array:
\begin{enumerate}
    \item For the range $[1, 3]$, the sum is $P[3] - P[0] = 10 - 3 = 7$.
    \item For the range $[0, 4]$, the sum is $P[4] = 15$ (since $i=0$).
    \item For the range $[2, 2]$, the sum is $P[2] - P[1] = 4 - 5 = -1$.
\end{enumerate}


Now let's make a seemingly small modification to the original problem. Now mixed into the queries are update operations that change the value of an element in the array $A$. For example, we might have a query that changes the value at index $k$ to some new value $v$. This is called a point update. Now since we have two different operations (queries and updates), we're going to consider both the average time per query and the average time per update, when evaluating our approaches.

If we go back to our naive approach, we can still answer range sum queries in $O(n)$ time by iterating through the array from index $l$ to index $r$. Updates can also be done in $O(1)$ time by simply changing the value at index $k$ to $v$. Thus, the average time per query is $O(n)$ and the average time per update is $O(1)$. So if we have a lot more updates than queries, this might be acceptable. However, this is generally not the case.

If we use the prefix sum array approach, we can still answer range sum queries in $O(1)$ time as before. However, updates are more complicated. If we change the value at index $k$ to $v$, we need to update all entries in the prefix sum array from index $k$ to index $n-1$ to reflect this change. This is because each of these entries includes the value at index $k$ in their sums. Thus, an update operation takes $O(n)$ time in the worst case. Therefore, the average time per query is $O(1)$ but the average time per update is $O(n)$. Since there generally are more queries than updates, this is usually better than the naive approach, but we can still do better.

Notice the issue with the prefix array is that when an update occures, we have to update many entries in the prefix array. This means that in terms of updating, the $i^{th}$ entry of the prefix array is dependent on all previous entries, which means that earlier entries have a lot of other entries depending on them. Our goal now is to reduce the number of entries that depend on any one entry, while still maintaining efficient query times.

To start, let's consider a different way of breaking up the array $A$. Instead of storing prefix sums, we can divide the array into ``blocks'' each of size $m$. Then, we will precompute the sum of each block and store these sums in a separate array $B$. Now, to answer a range sum query $[l, r]$, we can break the range into three parts: the partial block containing index $l$, the full blocks between $l$ and $r$, and the partial block containing index $r$. We can compute the sum of the full blocks using the precomputed sums in array $B$, and we can compute the sums of the partial blocks by iterating through the elements in those blocks. The time complexity for answering a query is $O(m + \frac{n}{m})$ since in the worst case we have to iterate through nearly a full block at the start and end of the range and the number of full blocks which is at most $\frac{n}{m}$. To update an element we simply update the value in array $A$ and also update the corresponding block sum in array $B$. Both can be done in $O(1)$ time in this case since the block sum can be updated by subtracting the old value and adding the new value but this approach works for operations other than addition unlike the prefix array where the operation has to be invertible. If we choose $m = \sqrt{n}$, then the time complexity for queries becomes $O(\sqrt{n} + \frac{n}{\sqrt{n}}=O(\sqrt{n}))$. This method is known as Square Root Decomposition. This approach is a significant improvement over both the naive and prefix array approaches when updates are involved but even though for most problems this is sufficient, it's pretty rare to see this data structure used in competitive programming since there are often better alternatives.


One example is the Fenwick Tree, also known as a Binary Indexed Tree (BIT). Instead of dividing the array into blocks like in Square Root Decomposition, Fenwick Trees takes the prefix sum approach but instead of storing the prefix sums directly, they use a tree structure to allow for the prefix sums to be computed efficiently while also allowing for efficient point updates.


\end{document}
